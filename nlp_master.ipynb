{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2157,"sourceType":"datasetVersion","datasetId":18}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-19T06:09:10.835921Z","iopub.execute_input":"2024-06-19T06:09:10.836645Z","iopub.status.idle":"2024-06-19T06:09:10.843977Z","shell.execute_reply.started":"2024-06-19T06:09:10.836617Z","shell.execute_reply":"2024-06-19T06:09:10.843173Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/working/state.db\n/kaggle/working/bert_cnn_results/model.safetensors\n/kaggle/working/bert_cnn_results/vocab.txt\n/kaggle/working/bert_cnn_results/special_tokens_map.json\n/kaggle/working/bert_cnn_results/tokenizer_config.json\n/kaggle/working/bert_cnn_results/config.json\n/kaggle/working/bert_cnn_results/tokenizer.json\n/kaggle/working/bert_cnn_results/checkpoint-9500/trainer_state.json\n/kaggle/working/bert_cnn_results/checkpoint-9500/model.safetensors\n/kaggle/working/bert_cnn_results/checkpoint-9500/vocab.txt\n/kaggle/working/bert_cnn_results/checkpoint-9500/special_tokens_map.json\n/kaggle/working/bert_cnn_results/checkpoint-9500/training_args.bin\n/kaggle/working/bert_cnn_results/checkpoint-9500/tokenizer_config.json\n/kaggle/working/bert_cnn_results/checkpoint-9500/rng_state.pth\n/kaggle/working/bert_cnn_results/checkpoint-9500/scheduler.pt\n/kaggle/working/bert_cnn_results/checkpoint-9500/config.json\n/kaggle/working/bert_cnn_results/checkpoint-9500/tokenizer.json\n/kaggle/working/bert_cnn_results/checkpoint-9500/optimizer.pt\n/kaggle/working/bert_cnn_results/checkpoint-10000/trainer_state.json\n/kaggle/working/bert_cnn_results/checkpoint-10000/model.safetensors\n/kaggle/working/bert_cnn_results/checkpoint-10000/vocab.txt\n/kaggle/working/bert_cnn_results/checkpoint-10000/special_tokens_map.json\n/kaggle/working/bert_cnn_results/checkpoint-10000/training_args.bin\n/kaggle/working/bert_cnn_results/checkpoint-10000/tokenizer_config.json\n/kaggle/working/bert_cnn_results/checkpoint-10000/rng_state.pth\n/kaggle/working/bert_cnn_results/checkpoint-10000/scheduler.pt\n/kaggle/working/bert_cnn_results/checkpoint-10000/config.json\n/kaggle/working/bert_cnn_results/checkpoint-10000/tokenizer.json\n/kaggle/working/bert_cnn_results/checkpoint-10000/optimizer.pt\n/kaggle/working/bert_cnn_results/best_model/model.safetensors\n/kaggle/working/bert_cnn_results/best_model/vocab.txt\n/kaggle/working/bert_cnn_results/best_model/special_tokens_map.json\n/kaggle/working/bert_cnn_results/best_model/tokenizer_config.json\n/kaggle/working/bert_cnn_results/best_model/config.json\n/kaggle/working/bert_cnn_results/best_model/tokenizer.json\n/kaggle/working/trans_blstm_model/checkpoint-500/trainer_state.json\n/kaggle/working/trans_blstm_model/checkpoint-500/model.safetensors\n/kaggle/working/trans_blstm_model/checkpoint-500/vocab.txt\n/kaggle/working/trans_blstm_model/checkpoint-500/special_tokens_map.json\n/kaggle/working/trans_blstm_model/checkpoint-500/training_args.bin\n/kaggle/working/trans_blstm_model/checkpoint-500/tokenizer_config.json\n/kaggle/working/trans_blstm_model/checkpoint-500/rng_state.pth\n/kaggle/working/trans_blstm_model/checkpoint-500/scheduler.pt\n/kaggle/working/trans_blstm_model/checkpoint-500/config.json\n/kaggle/working/trans_blstm_model/checkpoint-500/tokenizer.json\n/kaggle/working/trans_blstm_model/checkpoint-500/optimizer.pt\n/kaggle/working/trans_blstm_model/checkpoint-1000/trainer_state.json\n/kaggle/working/trans_blstm_model/checkpoint-1000/model.safetensors\n/kaggle/working/trans_blstm_model/checkpoint-1000/vocab.txt\n/kaggle/working/trans_blstm_model/checkpoint-1000/special_tokens_map.json\n/kaggle/working/trans_blstm_model/checkpoint-1000/training_args.bin\n/kaggle/working/trans_blstm_model/checkpoint-1000/tokenizer_config.json\n/kaggle/working/trans_blstm_model/checkpoint-1000/rng_state.pth\n/kaggle/working/trans_blstm_model/checkpoint-1000/scheduler.pt\n/kaggle/working/trans_blstm_model/checkpoint-1000/config.json\n/kaggle/working/trans_blstm_model/checkpoint-1000/tokenizer.json\n/kaggle/working/trans_blstm_model/checkpoint-1000/optimizer.pt\n/kaggle/working/trans_blstm_model/best_model/model.safetensors\n/kaggle/working/trans_blstm_model/best_model/vocab.txt\n/kaggle/working/trans_blstm_model/best_model/special_tokens_map.json\n/kaggle/working/trans_blstm_model/best_model/tokenizer_config.json\n/kaggle/working/trans_blstm_model/best_model/config.json\n/kaggle/working/trans_blstm_model/best_model/tokenizer.json\n/kaggle/working/roberta_results/best_model/model.safetensors\n/kaggle/working/roberta_results/best_model/vocab.txt\n/kaggle/working/roberta_results/best_model/special_tokens_map.json\n/kaggle/working/roberta_results/best_model/tokenizer_config.json\n/kaggle/working/roberta_results/best_model/config.json\n/kaggle/working/roberta_results/best_model/tokenizer.json\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers datasets scikit-learn torch","metadata":{"execution":{"iopub.status.busy":"2024-06-19T06:09:10.847074Z","iopub.execute_input":"2024-06-19T06:09:10.847363Z","iopub.status.idle":"2024-06-19T06:09:24.270117Z","shell.execute_reply.started":"2024-06-19T06:09:10.847336Z","shell.execute_reply":"2024-06-19T06:09:24.268993Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import BertModel, BertPreTrainedModel\nfrom torch import nn\n\nclass BERTCNN(BertPreTrainedModel):\n    def __init__(self, config):\n        super(BERTCNN, self).__init__(config)\n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", config=config)\n        self.conv = nn.Conv1d(in_channels=config.hidden_size, out_channels=128, kernel_size=5, padding=2)\n        self.pool = nn.AdaptiveMaxPool1d(1)\n        self.dropout = nn.Dropout(0.5)\n        self.classifier = nn.Linear(128, config.num_labels)\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds)\n        sequence_output = outputs[0]\n        sequence_output = sequence_output.permute(0, 2, 1)\n        x = self.conv(sequence_output)\n        x = self.pool(x).squeeze(-1)\n        x = self.dropout(x)\n        logits = self.classifier(x)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n\n        return (loss, logits) if loss is not None else logits","metadata":{"execution":{"iopub.status.busy":"2024-06-19T06:09:24.272617Z","iopub.execute_input":"2024-06-19T06:09:24.273021Z","iopub.status.idle":"2024-06-19T06:09:24.285720Z","shell.execute_reply.started":"2024-06-19T06:09:24.272973Z","shell.execute_reply":"2024-06-19T06:09:24.284767Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import BertModel, BertPreTrainedModel, BertConfig, AutoTokenizer, TrainingArguments, Trainer\n\nclass TransBLSTM(BertPreTrainedModel):\n    def __init__(self, config):\n        super(TransBLSTM, self).__init__(config)\n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", config=config)\n        self.blstm = nn.LSTM(config.hidden_size, config.hidden_size // 2, \n                             num_layers=1, bidirectional=True, batch_first=True)\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        self.dropout = nn.Dropout(0.5)\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n                position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n        bert_outputs = self.bert(input_ids, attention_mask=attention_mask, \n                                 token_type_ids=token_type_ids, position_ids=position_ids, \n                                 head_mask=head_mask, inputs_embeds=inputs_embeds)\n        \n        sequence_output = bert_outputs[0]\n        blstm_output, _ = self.blstm(sequence_output)\n        combined_output = self.layer_norm(sequence_output + blstm_output)\n        \n        pooled_output = combined_output[:, 0]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n        return (loss, logits) if loss is not None else logits\n","metadata":{"execution":{"iopub.status.busy":"2024-06-19T06:09:24.287025Z","iopub.execute_input":"2024-06-19T06:09:24.287290Z","iopub.status.idle":"2024-06-19T06:09:24.302172Z","shell.execute_reply.started":"2024-06-19T06:09:24.287261Z","shell.execute_reply":"2024-06-19T06:09:24.301290Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nclass RoBERTa:\n    def __init__(self, model_type='cardiffnlp/twitter-roberta-base-sentiment', num_labels=3):\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_type, num_labels=num_labels)\n\n    def get_model(self):\n        return self.model","metadata":{"execution":{"iopub.status.busy":"2024-06-19T06:09:24.304857Z","iopub.execute_input":"2024-06-19T06:09:24.305118Z","iopub.status.idle":"2024-06-19T06:09:24.316626Z","shell.execute_reply.started":"2024-06-19T06:09:24.305095Z","shell.execute_reply":"2024-06-19T06:09:24.315546Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!pip install transformers[torch] accelerate -U","metadata":{"execution":{"iopub.status.busy":"2024-06-19T06:09:24.318844Z","iopub.execute_input":"2024-06-19T06:09:24.319140Z","iopub.status.idle":"2024-06-19T06:09:37.713060Z","shell.execute_reply.started":"2024-06-19T06:09:24.319108Z","shell.execute_reply":"2024-06-19T06:09:37.712133Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nCollecting accelerate\n  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\nDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.30.1\n    Uninstalling accelerate-0.30.1:\n      Successfully uninstalled accelerate-0.30.1\nSuccessfully installed accelerate-0.31.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, BertConfig\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom datasets import Dataset\nimport torch\n\ndef load_data(file_path):\n    df = pd.read_csv(file_path, on_bad_lines='skip', nrows=5000)\n    df['Sentiment'] = df['Score'].apply(map_score_to_sentiment)\n    return train_test_split(df[['Text', 'Sentiment']], test_size=0.4, random_state=42)\n\ndef map_score_to_sentiment(score):\n    return 0 if score < 3 else (1 if score == 3 else 2)\n\ndef tokenize_data(tokenizer, texts, labels):\n    tokenized_inputs = tokenizer(texts.tolist(), padding=\"max_length\", truncation=True, max_length=512)\n    return Dataset.from_dict({**tokenized_inputs, 'labels': labels.tolist()})\n\n\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    return {\"f1\": f1_score(p.label_ids, preds, average='macro')}  # average could also be 'micro' or 'weighted'\n\ndef train_model(model, train_dataset, test_dataset, tokenizer, output_dir):\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        report_to=\"none\",\n        num_train_epochs=3,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=64,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir='./logs',\n        logging_steps=10,\n        save_total_limit=2,\n        save_steps=500,\n        eval_strategy=\"steps\",\n        eval_steps=500,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\" \n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset\n    )\n\n    trainer.train()\n\n    model_path = f\"{output_dir}/best_model\"\n    model.save_pretrained(model_path)\n    tokenizer.save_pretrained(model_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T06:09:37.714705Z","iopub.execute_input":"2024-06-19T06:09:37.715470Z","iopub.status.idle":"2024-06-19T06:09:37.728199Z","shell.execute_reply.started":"2024-06-19T06:09:37.715429Z","shell.execute_reply":"2024-06-19T06:09:37.727218Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = load_data('/kaggle/input/amazon-fine-food-reviews/Reviews.csv')\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ntrain_dataset = tokenize_data(tokenizer, train_df['Text'], train_df['Sentiment'])\ntest_dataset = tokenize_data(tokenizer, test_df['Text'], test_df['Sentiment'])\n\ndef data_stats(data_df):\n    num_reviews = data_df.shape[0]\n    print(f\"Total number of reviews: {num_reviews}\")\n\n    lengths = data_df['Text'].apply(len)\n    average_length = lengths.mean()\n    print(f\"Average review length: {average_length:.2f} characters\")\n\n    all_words = ' '.join(data_df['Text']).split()\n    vocab_size = len(set(all_words))\n    print(f\"Vocabulary size: {vocab_size}\")\n\n    median_length = lengths.median()\n    min_length = lengths.min()\n    max_length = lengths.max()\n    print(f\"Median review length: {median_length} characters\")\n    print(f\"Minimum review length: {min_length} characters\")\n    print(f\"Maximum review length: {max_length} characters\")\n\n\n    negative_count = len([negative for negative in data_df['Sentiment'] if negative == 0])\n    neutral_count = len([neutral for neutral in data_df['Sentiment'] if neutral == 1])\n    positive_count = len([positive for positive in data_df['Sentiment'] if positive == 2])\n    print(f'Negative Count: {negative_count}')\n    print(f'Neutral Count: {neutral_count}')\n    print(f'Positive Count: {positive_count}')\n\nprint(\"Training Data Statistics:\")\ndata_stats(train_df)\nprint(\"\\nTesting Data Statistics:\")\ndata_stats(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T06:09:37.729434Z","iopub.execute_input":"2024-06-19T06:09:37.730125Z","iopub.status.idle":"2024-06-19T06:09:41.996233Z","shell.execute_reply.started":"2024-06-19T06:09:37.730095Z","shell.execute_reply":"2024-06-19T06:09:41.995292Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb2860f4db9d4c4e8de6493b6f9422c7"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b52d7d49db2448faf64170498245a0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91d52211fb6e438482031367141ad390"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f84e27b9b434b6c9b2a0fcb35977b95"}},"metadata":{}},{"name":"stdout","text":"Training Data Statistics:\nTotal number of reviews: 3000\nAverage review length: 410.83 characters\nVocabulary size: 24277\nMedian review length: 294.0 characters\nMinimum review length: 57 characters\nMaximum review length: 5276 characters\nNegative Count: 461\nNeutral Count: 234\nPositive Count: 2305\n\nTesting Data Statistics:\nTotal number of reviews: 2000\nAverage review length: 406.47 characters\nVocabulary size: 18498\nMedian review length: 281.0 characters\nMinimum review length: 64 characters\nMaximum review length: 5160 characters\nNegative Count: 298\nNeutral Count: 161\nPositive Count: 1541\n","output_type":"stream"}]},{"cell_type":"code","source":"bert_model_type = 'bert-base-uncased'\nbert_cnn_config = BertConfig.from_pretrained(bert_model_type, num_labels=3)\nbert_cnn_model = BERTCNN(config=bert_cnn_config)\ntrain_model(bert_cnn_model, train_dataset, test_dataset, tokenizer, './bert_cnn_results')","metadata":{"execution":{"iopub.status.busy":"2024-06-19T06:09:41.997528Z","iopub.execute_input":"2024-06-19T06:09:41.998159Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"574d3155d0aa469587dee31995cddaa1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='474' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [474/564 07:02 < 01:20, 1.12 it/s, Epoch 2.52/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"bert_model_type = \"bert-base-uncased\"\ntrans_blstm_config = BertConfig.from_pretrained(bert_model_type, num_labels=3)\ntrans_blstm_model = TransBLSTM.from_pretrained(bert_model_type, config=trans_blstm_config)\ntrain_model(trans_blstm_model, train_dataset, test_dataset, tokenizer, \"./trans_blstm_model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roberta_model_type = 'cardiffnlp/twitter-roberta-base-sentiment'\nroberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_type)\nroberta_train_dataset = tokenize_data(roberta_tokenizer,  train_df['Text'], train_df['Sentiment'])\nroberta_test_dataset = tokenize_data(roberta_tokenizer,  test_df['Text'], test_df['Sentiment'])\n\nroberta_model = RoBERTa(model_type=roberta_model_type).get_model()\ntrain_model(roberta_model, roberta_train_dataset, roberta_test_dataset, tokenizer, './roberta_results')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_model(model, test_dataset):\n    trainer = Trainer(model=model)\n    result = trainer.predict(test_dataset)\n    prediction = np.argmax(result.predictions, axis=1)\n    return result, prediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_test_data(model_type, test_df):\n    tokenizer = AutoTokenizer.from_pretrained(model_type)\n    \n    def tokenize_function(examples):\n        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n    \n    test_dataset = Dataset.from_pandas(test_df).map(tokenize_function, batched=True)\n    \n    return test_dataset, tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_cnn_result, bert_cnn_preds = test_model(bert_cnn_model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trans_blstm_result, trans_blstm_preds = test_model(trans_blstm_model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roberta_result, roberta_preds = test_model(roberta_model, roberta_test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compare(model_result, model_preds, model_type):\n    print(model_type)\n    cases = ['negative', 'neutral', 'positive']\n\n    predictions_map = {\n        'negative': [],\n        'neutral': [],\n        'positive': [],\n    }\n    truth_map = {\n        'negative': [],\n        'neutral': [],\n        'positive': [],\n    }\n    falsy_map = {\n        'negative': [],\n        'neutral': [],\n        'positive': [],\n    }\n\n    for i, (result, preds) in enumerate(zip(model_result, model_preds)):\n        score = test_df['Sentiment'].iloc[i]\n        truth_map[cases[score]].append(i)\n        predictions_map[cases[preds]].append(i)\n        if score > 0 and preds == 0:\n            falsy_map[cases[0]].append(i)\n        elif score != 1 and preds == 1:\n            falsy_map[cases[1]].append(i)\n        elif score <2 and preds == 2:\n            falsy_map[cases[2]].append(i)\n\n\n    total_data = len(predictions_map[cases[0]]) + len(predictions_map[cases[1]]) + len(predictions_map[cases[2]])\n\n    print(\"Predictions\")\n    print(f'Negative:{len(predictions_map[cases[0]])} | Neutral: {len(predictions_map[cases[1]])} | Positive: {len(predictions_map[cases[2]])}')\n    print(\"============\\n\")\n    print(\"Truth\")\n    print(f'Negative:{len(truth_map[cases[0]])} | Neutral: {len(truth_map[cases[1]])} | Positive: {len(truth_map[cases[2]])}')\n    print(\"============\\n\")\n    print(\"False Positives\")\n    print(f'Negative:{len(falsy_map[cases[0]])} ({len(falsy_map[cases[0]])/len(truth_map[cases[0]])*100})| Neutral: {len(falsy_map[cases[1]])} ({len(falsy_map[cases[1]])/len(truth_map[cases[1]])*100})| Positive: {len(falsy_map[cases[2]])} ({len(falsy_map[cases[2]])/len(truth_map[cases[2]])*100})')\n    \n    print(\"============\\n\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare(bert_cnn_result.predictions, bert_cnn_preds,'bert-cnn')\ncompare(roberta_result.predictions, roberta_preds, roberta_model_type)\ncompare(trans_blstm_result.predictions, trans_blstm_preds, 'trans-blstm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}