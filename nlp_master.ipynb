{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2157,"sourceType":"datasetVersion","datasetId":18}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers datasets scikit-learn torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BertModel, BertPreTrainedModel\nfrom torch import nn\n\nclass BERTCNN(BertPreTrainedModel):\n    def __init__(self, config):\n        super(BERTCNN, self).__init__(config)\n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", config=config)\n        self.conv = nn.Conv1d(in_channels=config.hidden_size, out_channels=128, kernel_size=5, padding=2)\n        self.pool = nn.AdaptiveMaxPool1d(1)\n        self.dropout = nn.Dropout(0.5)\n        self.classifier = nn.Linear(128, config.num_labels)\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds)\n        sequence_output = outputs[0]\n        sequence_output = sequence_output.permute(0, 2, 1)\n        x = self.conv(sequence_output)\n        x = self.pool(x).squeeze(-1)\n        x = self.dropout(x)\n        logits = self.classifier(x)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n\n        return (loss, logits) if loss is not None else logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import BertModel, BertPreTrainedModel, BertConfig, AutoTokenizer, TrainingArguments, Trainer\n\nclass TransBLSTM(BertPreTrainedModel):\n    def __init__(self, config):\n        super(TransBLSTM, self).__init__(config)\n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", config=config)\n        self.blstm = nn.LSTM(config.hidden_size, config.hidden_size // 2, \n                             num_layers=1, bidirectional=True, batch_first=True)\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        self.dropout = nn.Dropout(0.5)\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n                position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n        bert_outputs = self.bert(input_ids, attention_mask=attention_mask, \n                                 token_type_ids=token_type_ids, position_ids=position_ids, \n                                 head_mask=head_mask, inputs_embeds=inputs_embeds)\n        \n        sequence_output = bert_outputs[0]\n        blstm_output, _ = self.blstm(sequence_output)\n        combined_output = self.layer_norm(sequence_output + blstm_output)\n        \n        pooled_output = combined_output[:, 0]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n        return (loss, logits) if loss is not None else logits\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nclass RoBERTa:\n    def __init__(self, model_type='cardiffnlp/twitter-roberta-base-sentiment', num_labels=3):\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_type, num_labels=num_labels)\n\n    def get_model(self):\n        return self.model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers[torch] accelerate -U","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, BertConfig\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom datasets import Dataset\nimport torch\n\ndef load_data(file_path):\n    df = pd.read_csv(file_path, on_bad_lines='skip', nrows=1000)\n    df['Sentiment'] = df['Score'].apply(map_score_to_sentiment)\n    return train_test_split(df[['Text', 'Sentiment']], test_size=0.4, random_state=42)\n\ndef map_score_to_sentiment(score):\n    return 0 if score < 3 else (1 if score == 3 else 2)\n\ndef tokenize_data(tokenizer, texts, labels):\n    tokenized_inputs = tokenizer(texts.tolist(), padding=\"max_length\", truncation=True, max_length=512)\n    return Dataset.from_dict({**tokenized_inputs, 'labels': labels.tolist()})\n\n\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    return {\"f1\": f1_score(p.label_ids, preds, average='macro')}\n\ndef train_model(model, train_dataset, test_dataset, tokenizer, output_dir):\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        report_to=\"none\",\n        num_train_epochs=3,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=64,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir='./logs',\n        logging_steps=10,\n        save_total_limit=2,\n        save_steps=500,\n        eval_strategy=\"steps\",\n        eval_steps=500,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\" \n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n\n    model_path = f\"{output_dir}/best_model\"\n    model.save_pretrained(model_path)\n    tokenizer.save_pretrained(model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = load_data('/kaggle/input/amazon-fine-food-reviews/Reviews.csv')\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ntrain_dataset = tokenize_data(tokenizer, train_df['Text'], train_df['Sentiment'])\ntest_dataset = tokenize_data(tokenizer, test_df['Text'], test_df['Sentiment'])\n\ndef data_stats(data_df):\n    num_reviews = data_df.shape[0]\n    print(f\"Total number of reviews: {num_reviews}\")\n\n    lengths = data_df['Text'].apply(len)\n    average_length = lengths.mean()\n    print(f\"Average review length: {average_length:.2f} characters\")\n\n    all_words = ' '.join(data_df['Text']).split()\n    vocab_size = len(set(all_words))\n    print(f\"Vocabulary size: {vocab_size}\")\n\n    median_length = lengths.median()\n    min_length = lengths.min()\n    max_length = lengths.max()\n    print(f\"Median review length: {median_length} characters\")\n    print(f\"Minimum review length: {min_length} characters\")\n    print(f\"Maximum review length: {max_length} characters\")\n\n\n    negative_count = len([negative for negative in data_df['Sentiment'] if negative == 0])\n    neutral_count = len([neutral for neutral in data_df['Sentiment'] if neutral == 1])\n    positive_count = len([positive for positive in data_df['Sentiment'] if positive == 2])\n    print(f'Negative Count: {negative_count}')\n    print(f'Neutral Count: {neutral_count}')\n    print(f'Positive Count: {positive_count}')\n\nprint(\"Training Data Statistics:\")\ndata_stats(train_df)\nprint(\"\\nTesting Data Statistics:\")\ndata_stats(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model_type = 'bert-base-uncased'\nbert_cnn_config = BertConfig.from_pretrained(bert_model_type, num_labels=3)\nbert_cnn_model = BERTCNN(config=bert_cnn_config)\ntrain_model(bert_cnn_model, train_dataset, test_dataset, tokenizer, './bert_cnn_results')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model_type = \"bert-base-uncased\"\ntrans_blstm_config = BertConfig.from_pretrained(bert_model_type, num_labels=3)\ntrans_blstm_model = TransBLSTM.from_pretrained(bert_model_type, config=trans_blstm_config)\ntrain_model(trans_blstm_model, train_dataset, test_dataset, tokenizer, \"./trans_blstm_model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roberta_model_type = 'cardiffnlp/twitter-roberta-base-sentiment'\nroberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_type)\nroberta_train_dataset = tokenize_data(roberta_tokenizer,  train_df['Text'], train_df['Sentiment'])\nroberta_test_dataset = tokenize_data(roberta_tokenizer,  test_df['Text'], test_df['Sentiment'])\n\nroberta_model = RoBERTa(model_type=roberta_model_type).get_model()\ntrain_model(roberta_model, roberta_train_dataset, roberta_test_dataset, tokenizer, './roberta_results')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_model(model, test_dataset):\n    trainer = Trainer(model=model)\n    result = trainer.predict(test_dataset)\n    prediction = np.argmax(result.predictions, axis=1)\n    return result, prediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_cnn_result, bert_cnn_preds = test_model(bert_cnn_model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trans_blstm_result, trans_blstm_preds = test_model(trans_blstm_model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roberta_result, roberta_preds = test_model(roberta_model, roberta_test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compare(model_result, model_preds, model_type):\n    print(model_type)\n    cases = ['negative', 'neutral', 'positive']\n\n    predictions_map = {\n        'negative': [],\n        'neutral': [],\n        'positive': [],\n    }\n    truth_map = {\n        'negative': [],\n        'neutral': [],\n        'positive': [],\n    }\n    falsy_map = {\n        'negative': [],\n        'neutral': [],\n        'positive': [],\n    }\n\n    for i, (result, preds) in enumerate(zip(model_result, model_preds)):\n        score = test_df['Sentiment'].iloc[i]\n        truth_map[cases[score]].append(i)\n        predictions_map[cases[preds]].append(i)\n        if score > 0 and preds == 0:\n            falsy_map[cases[0]].append(i)\n        elif score != 1 and preds == 1:\n            falsy_map[cases[1]].append(i)\n        elif score <2 and preds == 2:\n            falsy_map[cases[2]].append(i)\n\n\n    total_data = len(predictions_map[cases[0]]) + len(predictions_map[cases[1]]) + len(predictions_map[cases[2]])\n\n    print(\"Predictions\")\n    print(f'Negative:{len(predictions_map[cases[0]])} | Neutral: {len(predictions_map[cases[1]])} | Positive: {len(predictions_map[cases[2]])}')\n    print(\"============\\n\")\n    print(\"Truth\")\n    print(f'Negative:{len(truth_map[cases[0]])} | Neutral: {len(truth_map[cases[1]])} | Positive: {len(truth_map[cases[2]])}')\n    print(\"============\\n\")\n    print(\"False Positives\")\n    print(f'Negative:{len(falsy_map[cases[0]])} ({len(falsy_map[cases[0]])/len(truth_map[cases[0]])*100})| Neutral: {len(falsy_map[cases[1]])} ({len(falsy_map[cases[1]])/len(truth_map[cases[1]])*100})| Positive: {len(falsy_map[cases[2]])} ({len(falsy_map[cases[2]])/len(truth_map[cases[2]])*100})')\n    \n    print(\"============\\n\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare(bert_cnn_result.predictions, bert_cnn_preds,'bert-cnn')\ncompare(roberta_result.predictions, roberta_preds, roberta_model_type)\ncompare(trans_blstm_result.predictions, trans_blstm_preds, 'trans-blstm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}