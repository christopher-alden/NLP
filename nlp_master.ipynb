{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2157,"sourceType":"datasetVersion","datasetId":18}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-18T10:13:46.883040Z","iopub.execute_input":"2024-06-18T10:13:46.883573Z","iopub.status.idle":"2024-06-18T10:13:46.902248Z","shell.execute_reply.started":"2024-06-18T10:13:46.883537Z","shell.execute_reply":"2024-06-18T10:13:46.900107Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/input/stsgold-dataset/sts_gold_tweet.csv\n/kaggle/input/amazon-fine-food-reviews/hashes.txt\n/kaggle/input/amazon-fine-food-reviews/Reviews.csv\n/kaggle/input/amazon-fine-food-reviews/database.sqlite\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers datasets scikit-learn torch","metadata":{"execution":{"iopub.status.busy":"2024-06-18T10:24:23.907080Z","iopub.execute_input":"2024-06-18T10:24:23.907990Z","iopub.status.idle":"2024-06-18T10:24:37.669568Z","shell.execute_reply.started":"2024-06-18T10:24:23.907957Z","shell.execute_reply":"2024-06-18T10:24:37.668319Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import BertModel, BertPreTrainedModel\nfrom torch import nn\n\nclass BERTCNN(BertPreTrainedModel):\n    def __init__(self, config):\n        super(BERTCNN, self).__init__(config)\n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", config=config)\n        self.conv = nn.Conv1d(in_channels=config.hidden_size, out_channels=128, kernel_size=5, padding=2)\n        self.pool = nn.AdaptiveMaxPool1d(1)\n        self.dropout = nn.Dropout(0.5)\n        self.classifier = nn.Linear(128, config.num_labels)\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds)\n        sequence_output = outputs[0]\n        sequence_output = sequence_output.permute(0, 2, 1)\n        x = self.conv(sequence_output)\n        x = self.pool(x).squeeze(-1)\n        x = self.dropout(x)\n        logits = self.classifier(x)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n\n        return (loss, logits) if loss is not None else logits","metadata":{"execution":{"iopub.status.busy":"2024-06-18T10:24:43.020895Z","iopub.execute_input":"2024-06-18T10:24:43.021310Z","iopub.status.idle":"2024-06-18T10:24:43.031653Z","shell.execute_reply.started":"2024-06-18T10:24:43.021279Z","shell.execute_reply":"2024-06-18T10:24:43.030483Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import RobertaModel\n\nclass RoBERTa(nn.Module):\n    def __init__(self, model_type):\n        super(RoBERTa, self).__init__()\n        self.roberta = RobertaModel.from_pretrained(model_type)\n\n    def forward(self, input_ids, attention_mask=None):\n        return self.roberta(input_ids=input_ids, attention_mask=attention_mask)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T12:40:39.501237Z","iopub.execute_input":"2024-06-18T12:40:39.501972Z","iopub.status.idle":"2024-06-18T12:40:39.507836Z","shell.execute_reply.started":"2024-06-18T12:40:39.501943Z","shell.execute_reply":"2024-06-18T12:40:39.506817Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"code","source":"!pip install transformers[torch] accelerate -U","metadata":{"execution":{"iopub.status.busy":"2024-06-18T10:25:22.133818Z","iopub.execute_input":"2024-06-18T10:25:22.134602Z","iopub.status.idle":"2024-06-18T10:25:35.469837Z","shell.execute_reply.started":"2024-06-18T10:25:22.134569Z","shell.execute_reply":"2024-06-18T10:25:35.468531Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nCollecting accelerate\n  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\nDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.30.1\n    Uninstalling accelerate-0.30.1:\n      Successfully uninstalled accelerate-0.30.1\nSuccessfully installed accelerate-0.31.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, BertConfig\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nimport torch\n\ndef map_score_to_sentiment(score):\n    if score < 3:\n      return 0  # Negative\n    elif score == 3:\n      return 1  # Neutral\n    else:\n      return 2  # Positive\n            \ndef load_data():\n    df = pd.read_csv('/kaggle/input/amazon-fine-food-reviews/Reviews.csv', on_bad_lines='warn', nrows=100000)\n    df = df.head(100000)\n    print(df.shape[0], \"Data\")\n\n    df['Sentiment'] = df['Score'].apply(map_score_to_sentiment)\n\n    train_texts, test_texts, train_labels, test_labels = train_test_split(\n      df['Text'], df['Sentiment'], test_size=0.4, random_state=42)\n\n    train_df = pd.DataFrame({'text': train_texts, 'label': train_labels})\n    test_df = pd.DataFrame({'text': test_texts, 'label': test_labels})\n\n    return train_df, test_df\n\ndef tokenize_data(model_type, train_df, test_df):\n    tokenizer = AutoTokenizer.from_pretrained(model_type)\n\n    def tokenize_function(examples):\n        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n\n    train_dataset = Dataset.from_pandas(train_df).map(\n        tokenize_function, batched=True)\n    test_dataset = Dataset.from_pandas(test_df).map(\n        tokenize_function, batched=True)\n\n    return train_dataset, test_dataset, tokenizer\n\ndef train_model(model, train_dataset, test_dataset, tokenizer, output_dir):\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        report_to=\"none\",\n        num_train_epochs=3,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=64,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir='./logs',\n        logging_steps=10,\n        save_total_limit=2,\n        save_steps=500,\n        eval_strategy=\"steps\",\n        eval_steps=500,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        tokenizer=tokenizer\n    )\n\n    trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:04:15.209789Z","iopub.execute_input":"2024-06-18T13:04:15.210231Z","iopub.status.idle":"2024-06-18T13:04:15.224834Z","shell.execute_reply.started":"2024-06-18T13:04:15.210198Z","shell.execute_reply":"2024-06-18T13:04:15.223867Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = load_data()\nprint(train_df.shape[0])\nprint(test_df.shape[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-18T15:38:53.613493Z","iopub.execute_input":"2024-06-18T15:38:53.613859Z","iopub.status.idle":"2024-06-18T15:38:54.435196Z","shell.execute_reply.started":"2024-06-18T15:38:53.613830Z","shell.execute_reply":"2024-06-18T15:38:54.434293Z"},"trusted":true},"execution_count":182,"outputs":[{"name":"stdout","text":"100000 Data\n60000\n40000\n","output_type":"stream"}]},{"cell_type":"code","source":"bert_model_type = 'bert-base-uncased'\nbert_cnn_config = BertConfig.from_pretrained(bert_model_type, num_labels=3)\ntrain_dataset, test_dataset, tokenizer = tokenize_data(bert_model_type, train_df, test_df)\nbert_cnn_model = BERTCNN(config=bert_cnn_config)\ntrain_model(bert_cnn_model, train_dataset, test_dataset, tokenizer, './bert_cnn_results')","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:56:51.291759Z","iopub.execute_input":"2024-06-18T13:56:51.292454Z","iopub.status.idle":"2024-06-18T15:37:17.121876Z","shell.execute_reply.started":"2024-06-18T13:56:51.292422Z","shell.execute_reply":"2024-06-18T15:37:17.121082Z"},"trusted":true},"execution_count":178,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/60000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86d3c92c1bb040c786e34f127cd2712a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"983c363274b545eea815484346bb2ad7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='11250' max='11250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [11250/11250 1:39:58, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.479600</td>\n      <td>0.522757</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.363600</td>\n      <td>0.362736</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.457200</td>\n      <td>0.352290</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.455100</td>\n      <td>0.310538</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.267300</td>\n      <td>0.324507</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.278000</td>\n      <td>0.353920</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.386400</td>\n      <td>0.314407</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.149300</td>\n      <td>0.386761</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.191300</td>\n      <td>0.346772</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.253300</td>\n      <td>0.312312</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.260100</td>\n      <td>0.338966</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.261400</td>\n      <td>0.324878</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.197100</td>\n      <td>0.326446</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.262100</td>\n      <td>0.308367</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.242400</td>\n      <td>0.303756</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.114500</td>\n      <td>0.458293</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.147100</td>\n      <td>0.432451</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.040100</td>\n      <td>0.416073</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.078500</td>\n      <td>0.436183</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.184500</td>\n      <td>0.425374</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.177000</td>\n      <td>0.427337</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.151000</td>\n      <td>0.415614</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"roberta_model_type = 'cardiffnlp/twitter-roberta-base-sentiment'\ntrain_dataset, test_dataset, tokenizer = tokenize_data(roberta_model_type, train_df, test_df)\nroberta_model = RoBERTa(model_type=roberta_model_type)\nif torch.cuda.is_available():\n    roberta_model.cuda()\nroberta_model = roberta.get_model()\ntrain_model(roberta_model, train_dataset, test_dataset, tokenizer, './roberta_results')","metadata":{"execution":{"iopub.status.busy":"2024-06-18T18:17:33.531798Z","iopub.execute_input":"2024-06-18T18:17:33.532114Z","iopub.status.idle":"2024-06-18T18:17:33.559065Z","shell.execute_reply.started":"2024-06-18T18:17:33.532089Z","shell.execute_reply":"2024-06-18T18:17:33.557818Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m roberta_model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcardiffnlp/twitter-roberta-base-sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m train_dataset, test_dataset, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_data\u001b[49m(roberta_model_type, train_df, test_df)\n\u001b[1;32m      3\u001b[0m roberta_model \u001b[38;5;241m=\u001b[39m RoBERTa(model_type\u001b[38;5;241m=\u001b[39mroberta_model_type)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n","\u001b[0;31mNameError\u001b[0m: name 'tokenize_data' is not defined"],"ename":"NameError","evalue":"name 'tokenize_data' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-06-18T15:38:16.782229Z","iopub.execute_input":"2024-06-18T15:38:16.782903Z","iopub.status.idle":"2024-06-18T15:38:16.788114Z","shell.execute_reply.started":"2024-06-18T15:38:16.782874Z","shell.execute_reply":"2024-06-18T15:38:16.787201Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"code","source":"def test_model(model, test_dataset):\n    trainer = Trainer(model=model)\n    result = trainer.predict(test_dataset)\n    prediction = np.argmax(result.predictions, axis=1)\n    return result, prediction","metadata":{"execution":{"iopub.status.busy":"2024-06-18T15:38:18.962546Z","iopub.execute_input":"2024-06-18T15:38:18.962906Z","iopub.status.idle":"2024-06-18T15:38:18.968374Z","shell.execute_reply.started":"2024-06-18T15:38:18.962877Z","shell.execute_reply":"2024-06-18T15:38:18.967281Z"},"trusted":true},"execution_count":180,"outputs":[]},{"cell_type":"code","source":"def tokenize_test_data(model_type, test_df):\n    tokenizer = AutoTokenizer.from_pretrained(model_type)\n    \n    def tokenize_function(examples):\n        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n    \n    test_dataset = Dataset.from_pandas(test_df).map(tokenize_function, batched=True)\n    \n    return test_dataset, tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-06-18T11:43:16.728704Z","iopub.execute_input":"2024-06-18T11:43:16.729456Z","iopub.status.idle":"2024-06-18T11:43:16.734933Z","shell.execute_reply.started":"2024-06-18T11:43:16.729426Z","shell.execute_reply":"2024-06-18T11:43:16.734034Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"bert_model_type = 'bert-base-uncased'\ntest_dataset, tokenizer = tokenize_test_data(bert_model_type, test_df)\nbert_cnn_result, bert_cnn_preds = test_model(bert_cnn_model, test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T15:39:14.148691Z","iopub.execute_input":"2024-06-18T15:39:14.149097Z","iopub.status.idle":"2024-06-18T15:42:25.930880Z","shell.execute_reply.started":"2024-06-18T15:39:14.149068Z","shell.execute_reply":"2024-06-18T15:42:25.930115Z"},"trusted":true},"execution_count":184,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1adf0348b19c4421abdf5428239aa3d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"roberta_model_type = 'cardiffnlp/twitter-roberta-base-sentiment'\ntest_dataset, tokenizer = tokenize_test_data(roberta_model_type, test_df)\nroberta_result, roberta_preds = test_model(roberta_model, test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T12:49:08.475927Z","iopub.execute_input":"2024-06-18T12:49:08.476356Z","iopub.status.idle":"2024-06-18T12:49:27.521735Z","shell.execute_reply.started":"2024-06-18T12:49:08.476328Z","shell.execute_reply":"2024-06-18T12:49:27.520826Z"},"trusted":true},"execution_count":146,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b232bfa00e0a4ed2bf21010ece4f99f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"def compare(model_result, model_preds, model_type):\n    print(model_type)\n    cases = ['negative', 'neutral', 'positive']\n\n    predictions_map = {\n        'negative': [],\n        'neutral': [],\n        'positive': [],\n    }\n    truth_map = {\n        'negative': [],\n        'neutral': [],\n        'positive': [],\n    }\n    falsy_map = {\n        'negative': [],\n        'neutral': [],\n        'positive': [],\n    }\n\n    for i, (result, preds) in enumerate(zip(model_result, model_preds)):\n        score = test_df['label'].iloc[i]\n        truth_map[cases[score]].append(i)\n        predictions_map[cases[preds]].append(i)\n        if score > 0 and preds == 0:\n            falsy_map[cases[0]].append(i)\n        elif score != 1 and preds == 1:\n            falsy_map[cases[1]].append(i)\n        elif score <2 and preds == 2:\n            falsy_map[cases[2]].append(i)\n\n\n    total_data = len(predictions_map[cases[0]]) + len(predictions_map[cases[1]]) + len(predictions_map[cases[2]])\n\n    print(\"Predictions\")\n    print(f'Negative:{len(predictions_map[cases[0]])} | Neutral: {len(predictions_map[cases[1]])} | Positive: {len(predictions_map[cases[2]])}')\n    print(\"============\\n\")\n    print(\"Truth\")\n    print(f'Negative:{len(truth_map[cases[0]])} | Neutral: {len(truth_map[cases[1]])} | Positive: {len(truth_map[cases[2]])}')\n    print(\"============\\n\")\n    print(\"False Positives\")\n    print(f'Negative:{len(falsy_map[cases[0]])} ({len(falsy_map[cases[0]])/len(truth_map[cases[0]])*100})| Neutral: {len(falsy_map[cases[1]])} ({len(falsy_map[cases[1]])/len(truth_map[cases[1]])*100})| Positive: {len(falsy_map[cases[2]])} ({len(falsy_map[cases[2]])/len(truth_map[cases[2]])*100})')\n    \n    print(\"============\\n\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-06-18T15:43:06.979699Z","iopub.execute_input":"2024-06-18T15:43:06.980574Z","iopub.status.idle":"2024-06-18T15:43:06.996725Z","shell.execute_reply.started":"2024-06-18T15:43:06.980517Z","shell.execute_reply":"2024-06-18T15:43:06.995674Z"},"trusted":true},"execution_count":185,"outputs":[]},{"cell_type":"code","source":"compare(bert_cnn_result.predictions, bert_cnn_preds,'bert-cnn')\n# compare(roberta_result.predictions, roberta_preds, roberta_model_type)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T18:39:02.713268Z","iopub.execute_input":"2024-06-18T18:39:02.713636Z","iopub.status.idle":"2024-06-18T18:39:02.738029Z","shell.execute_reply.started":"2024-06-18T18:39:02.713607Z","shell.execute_reply":"2024-06-18T18:39:02.736778Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompare\u001b[49m(bert_cnn_result\u001b[38;5;241m.\u001b[39mpredictions, bert_cnn_preds,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-cnn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# compare(roberta_result.predictions, roberta_preds, roberta_model_type)\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'compare' is not defined"],"ename":"NameError","evalue":"name 'compare' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-06-18T18:17:59.188555Z","iopub.execute_input":"2024-06-18T18:17:59.189315Z","iopub.status.idle":"2024-06-18T18:17:59.219645Z","shell.execute_reply.started":"2024-06-18T18:17:59.189276Z","shell.execute_reply":"2024-06-18T18:17:59.217430Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompare\u001b[49m(bert_cnn_result\u001b[38;5;241m.\u001b[39mpredictions, bert_cnn_preds,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-cnn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'compare' is not defined"],"ename":"NameError","evalue":"name 'compare' is not defined","output_type":"error"}]}]}